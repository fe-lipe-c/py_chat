{"Chat1": {"chat_history": [{"role": "user", "content": "The following code is a chat interface that interacts with the OpenAi API. Please, write a github README.md for this project:\n\nimport streamlit as st\nimport openai\nimport config as cfg\nimport json\nimport os\n\nst.set_page_config(layout=\"wide\")\n\nst.markdown(\n    \"\"\"<style>.block-container{max-width: 86rem !important;}</style>\"\"\",\n    unsafe_allow_html=True,\n)\n\nCHAT_SAVE_FILE = \"chats_st.json\"\n\n\ndef save_chats_to_file(chats_st):\n    # Exclude streaming and generator states from the saved chats\n    chats_to_save = {\n        chat_name: {\n            key: value\n            for key, value in chat.items()\n            if key not in (\"streaming\", \"generator\")\n        }\n        for chat_name, chat in chats_st.items()\n    }\n    with open(CHAT_SAVE_FILE, \"w\") as f:\n        json.dump(chats_to_save, f)\n\n\ndef load_chats_from_file():\n    if os.path.exists(CHAT_SAVE_FILE):\n        with open(CHAT_SAVE_FILE, \"r\") as f:\n            chats_st = json.load(f)\n        for chat_name in chats_st:\n            chats_st[chat_name][\"streaming\"] = False\n            chats_st[chat_name][\"generator\"] = None\n\n        return chats_st\n\n    return {}\n\n\ndef init_session():\n    # Load saved chats from file\n    saved_chats = load_chats_from_file()\n\n    # Session state initialization\n    if \"chats\" not in st.session_state:\n        st.session_state.chats = saved_chats\n    if \"params\" not in st.session_state:\n        st.session_state.params = dict()\n    if \"current_chat\" not in st.session_state:\n        st.session_state.current_chat = None\n\n\ndef new_chat(chat_name):\n    if chat_name not in st.session_state.chats:\n        st.session_state.chats[chat_name] = {\n            \"chat_history\": [],\n            \"streaming\": False,\n            \"generator\": None,\n        }\n    st.session_state.current_chat = chat_name\n    # Save chats to file\n    save_chats_to_file(st.session_state.chats)\n\n\ndef chat(\n    messages,\n    n=1,\n    stream=True,\n):\n    \"\"\"\n    Generate chat responses using the OpenAI API.\n\n    Args:\n        messages (list): A list of messages to be processed.\n        max_tokens (int): The maximum length of the output message.\n        temperature (float, optional): Controls the randomness of the model's\n        output. Defaults to 1.\n        n (int, optional): The number of completions to generate for each prompt.\n        Defaults to 1.\n        model (str, optional): The model to use for generating responses. Defaults\n        to \"gpt-3.5-turbo-16k\".\n        stream (bool, optional): Whether to stream the output. Defaults to False.\n\n    Yields:\n        str: The generated response.\n    \"\"\"\n\n    completion = openai.chat.completions.create(\n        model=st.session_state[\"params\"][\"model\"],\n        messages=messages,\n        max_tokens=st.session_state[\"params\"][\"max_tokens\"],\n        temperature=st.session_state[\"params\"][\"temperature\"],\n        n=n,\n        stream=stream,\n    )\n    for chunk in completion:\n        try:\n            yield chunk.choices[0].delta.content if chunk.choices[\n                0\n            ].finish_reason != \"stop\" else \"\"\n        except:\n            yield \"error!\"\n\n\ndef render_sidebar(models_list):\n    with st.sidebar:\n        st.title(\"Chat Configuration\")\n        # New chat button\n        if st.button(\"New Chat\"):\n            chat_name = f\"Chat{len(st.session_state.chats) + 1}\"\n            new_chat(chat_name)\n\n        # Chat selection\n        chat_names = list(st.session_state.chats.keys())\n        st.session_state.current_chat = st.selectbox(\n            \"Select a chat\",\n            chat_names,\n            index=0\n            if st.session_state.current_chat is None\n            else chat_names.index(st.session_state.current_chat),\n        )\n\n        # GPT Model Configuration\n        st.session_state.params[\"model\"] = st.selectbox(\n            \"Select a model\",\n            models_list,\n            index=0,\n        )\n        st.session_state.params[\"temperature\"] = st.slider(\n            \"Temperature\", min_value=0.0, max_value=1.0, value=0.5, step=0.01\n        )\n        st.session_state.params[\"max_tokens\"] = st.slider(\n            \"Max Tokens\", min_value=1, max_value=2048, value=1000, step=1\n        )\n\n\ndef run_chat_interface():\n    current_chat = st.session_state.current_chat\n    if current_chat is None:\n        st.warning(\"Please create a new chat or select an existing one.\")\n        return\n\n    chat_data = st.session_state.chats[current_chat]\n    chat_history = chat_data[\"chat_history\"]\n    create_chat_area(chat_history)\n\n    # Chat controls\n    user_input = st.chat_input(\"Ask something\")\n\n    # Handle user input and generate assistant response\n    if user_input or chat_data[\"streaming\"]:\n        process_user_input(user_input, chat_data)\n\n\ndef create_chat_area(chat_history):\n    # Display the chat history\n    for c in chat_history:\n        role = c[\"role\"]\n        with st.chat_message(role):\n            st.write(c[\"content\"])\n\n\ndef process_user_input(user_input, chat_data):\n    # Process the user input and generate assistant response\n    chat_history = chat_data[\"chat_history\"]\n    if user_input:\n        chat_history.append({\"role\": \"user\", \"content\": user_input})\n        gpt_answer = chat(chat_history)\n        chat_data[\"generator\"] = gpt_answer\n        chat_data[\"streaming\"] = True\n        chat_history.append({\"role\": \"assistant\", \"content\": \"\"})\n        # Save chats to file\n        save_chats_to_file(st.session_state.chats)\n    else:\n        update_assistant_response(chat_data)\n\n    st.rerun()\n\n\ndef update_assistant_response(chat_data):\n    try:\n        chunk = next(chat_data[\"generator\"])\n        chat_data[\"chat_history\"][-1][\"content\"] += chunk\n        # Save chats to file\n        save_chats_to_file(st.session_state.chats)\n        st.rerun()\n    except StopIteration:\n        chat_data[\"streaming\"] = False\n        # Save chats to file\n        save_chats_to_file(st.session_state.chats)\n        st.rerun()\n\n\ndef main():\n    models_list = [\"gpt-4-1106-preview\", \"gpt-4\", \"gpt-4-32k\", \"gpt-3.5-turbo-1106\"]\n    init_session()\n    render_sidebar(models_list)\n    run_chat_interface()\n\n\nif __name__ == \"__main__\":\n    main()\n"}, {"role": "assistant", "content": "# OpenAI Chat Interface\n\nThis project provides a simple chat interface that interacts with the OpenAI API using Streamlit. Users can start a new chat, select an existing chat, and configure the GPT model parameters to generate responses from the model in a conversational context.\n\n## Features\n\n- **Chat Management**: Create new chats or select from existing ones.\n- **Model Configuration**: Select from a list of GPT models and adjust parameters such as temperature and max tokens.\n- **Persistent Chat History**: Chat histories are saved to a JSON file, allowing for persistence across sessions.\n- **Streamlit Interface**: A user-friendly web interface built with Streamlit for easy interaction.\n\n## Installation\n\nBefore you begin, ensure you have Python installed on your system. Then, follow these steps:\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/your-username/openai-chat-interface.git\n   cd openai-chat-interface\n   ```\n\n2. Install the required dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Set up your OpenAI API key:\n   - Create a `config.py` file in the root directory.\n   - Add the following line with your OpenAI API key:\n     ```python\n     OPENAI_API_KEY = \"your-api-key\"\n     ```\n\n## Usage\n\nTo run the chat interface, execute the following command in your terminal:\n\n```bash\nstreamlit run app.py\n```\n\nThis will start a local server and open the Streamlit web interface in your default web browser.\n\n## Configuration\n\nYou can configure the following parameters from the Streamlit sidebar:\n\n- **Select a chat**: Choose an existing chat or start a new one.\n- **Select a model**: Choose the GPT model you want to use for generating responses.\n- **Temperature**: Control the randomness of the model output.\n- **Max Tokens**: Set the maximum length of the output message.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a pull request or create an issue if you have any ideas, suggestions, or find any bugs.\n\n## License\n\nThis project is licensed under the MIT License. See the `LICENSE` file for details.\n\n## Disclaimer\n\nThis software is not officially associated with OpenAI. It is an independent project that utilizes the OpenAI API for educational purposes.\n\n## Acknowledgments\n\n- OpenAI for providing the GPT models and API.\n- Streamlit for the powerful and easy-to-use app framework.\n\n---\n\n**Note**: Don't forget to abide by the OpenAI usage policies and guidelines when interacting with the API.\n\nEnjoy building and chatting with your AI-powered assistant!\n\n---\n\n*This README is a template and should be customized to fit the specifics of your project.*"}]}}